{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjelammcgraw/LangChain-Agentic-RAG-with-LangGraph-LangSmith/blob/main/13_Agentic_RAG_powered_by_LangChain_with_LangGraph_and_LangSmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n"
      ],
      "metadata": {
        "id": "gJXW_DgiSebM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n"
      ],
      "metadata": {
        "id": "e7pQDUhUnIo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n"
      ],
      "metadata": {
        "id": "3_fLDElOVoop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaVwN269EttM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c86dec-a934-4973-efd2-1e347e36cab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Variables\n"
      ],
      "metadata": {
        "id": "wujPjGJuoPwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "1bfc4021-1557-4e85-b660-1014dc856d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "a36487b6-f99d-4b58-cea7-2f5e9dfe8c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our Tool Belt\n",
        "\n"
      ],
      "metadata": {
        "id": "sBRyQmEAVzua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    ArxivQueryRun(),\n",
        "    DuckDuckGoSearchRun()\n",
        "]"
      ],
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actioning with Tools\n"
      ],
      "metadata": {
        "id": "1FdOjEslXdRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ],
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n"
      ],
      "metadata": {
        "id": "VI-C669ZYVI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### ✅ Answer:\n",
        "\n",
        "The model assesses the context of the users' input to understand the intent and specific requirements of the request."
      ],
      "metadata": {
        "id": "ERzuGo6W18Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. User submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ],
      "metadata": {
        "id": "_296Ub96Z_H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n"
      ],
      "metadata": {
        "id": "vWsMhfO9grLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ],
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ],
      "metadata": {
        "id": "b8CjRlbVmRpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ],
      "metadata": {
        "id": "BUsfGoSpoF9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ],
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ],
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Cycling is infinite. However, we can use for loops to define a number of fixed iterations. We can use a conditional check within the loop to exit once a certain condition is met, or store the cycle limit in an external configuration file or database. We can use environment variables to set the cycle limit or implement monitoring of system resources. We can set a maximum time limit for the loop to run, or allow the user to define the limit either at the start of the process or dynamically adjust during runtime."
      ],
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "e3d929e2-b0d9-4409-9bce-8eddff419186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 171, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. Retrieval Augmented Generation. Retrieval Augmented Generation (RAG) is based on research produced by the Meta team to advance the natural language processing capabilities of large language models. Meta\\'s research proposed combining retriever and generator components to make language models more intelligent and accurate for generating text in a human voice and tone, which is also commonly ... RAG (Retrieval-Augmented Generation) is a pivotal technique in LLMs (Large Language Models), designed to mitigate errors or \\'hallucinations.\\' It achieves this by basing model responses on ... The beauty of RAG lies in its ability to enable a language model to draw upon and leverage your own data to generate responses. While base models are traditionally trained on specific, point-in-time data, ensuring their effectiveness in performing tasks and adapting to the desired domain, they can struggle when faced with newer or current data. Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren\\'t included in the models\\' original ...', name='duckduckgo_search'),\n",
              "  AIMessage(content=\"RAG, which stands for Retrieval-Augmented Generation, is a technique used in Large Language Models (LLMs) to improve their natural language processing capabilities. It combines retriever and generator components to make language models more intelligent and accurate in generating text in a human voice and tone. RAG helps mitigate errors or 'hallucinations' in LLMs by basing model responses on retrieved information. It enables language models to draw upon and leverage specific data sources to generate responses, making them more effective in adapting to newer or current data.\\n\\nRAG broke onto the scene as a pivotal technique in LLMs to enhance their performance and accuracy in generating text.\", response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 484, 'total_tokens': 618}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n"
      ],
      "metadata": {
        "id": "DBHnUtLSscRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "e99cf04d-6976-4d9f-befc-51dd97454d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA in Machine Learning\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 193, 'total_tokens': 212}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2023-12-31\\nTitle: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nAuthors: Dipankar Sarkar\\nSummary: This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology, economy, and\\nlaw, offering a comprehensive solution to the complex challenges of today's AI\\nlandscape.\\n\\nPublished: 2024-02-08\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnolog\", name='arxiv'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1162, 'total_tokens': 1179}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Published: 2016-02-19\\nTitle: 8-Bit Approximations for Parallelism in Deep Learning\\nAuthors: Tim Dettmers\\nSummary: The creation of practical deep learning data-products often requires\\nparallelization across processors and computers to make deep learning feasible\\non large data sets, but bottlenecks in communication bandwidth make it\\ndifficult to attain good speedups through parallelism. Here we develop and test\\n8-bit approximation algorithms which make better use of the available bandwidth\\nby compressing 32-bit gradients and nonlinear activations to 8-bit\\napproximations. We show that these approximations do not decrease predictive\\nperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelism\\nand provide a data transfer speedup of 2x relative to 32-bit parallelism. We\\nbuild a predictive model for speedups based on our experimental data, verify\\nits validity on known speedup data, and show that we can obtain a speedup of\\n50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We\\ncompare our data types with other methods and show that 8-bit approximations\\nachieve state-of-the-art speedups for model parallelism. Thus 8-bit\\napproximation is an efficient method to parallelize convolutional networks on\\nvery large systems of GPUs.\\n\\nPublished: 2019-08-23\\nTitle: Sparse Networks from Scratch: Faster Training without Losing Performance\\nAuthors: Tim Dettmers, Luke Zettlemoyer\\nSummary: We demonstrate the possibility of what we call sparse learning: accelerated\\ntraining of deep neural networks that maintain sparse weights throughout\\ntraining while achieving dense performance levels. We accomplish this by\\ndeveloping sparse momentum, an algorithm which uses exponentially smoothed\\ngradients (momentum) to identify layers and weights which reduce the error\\nefficiently. Sparse momentum redistributes pruned weights across layers\\naccording to the mean momentum magnitude of each layer. Within a layer, sparse\\nmomentum grows weights according to the momentum magnitude of zero-valued\\nweights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10,\\nand ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared\\nto other sparse algorithms. Furthermore, we show that sparse momentum reliably\\nreproduces dense performance levels while providing up to 5.61x faster\\ntraining. In our analysis, ablations show that the benefits of momentum\\nredistribution and growth increase with the depth and size of the network.\\nAdditionally, we find that sparse momentum is insensitive to the choice of its\\nhyperparameters suggesting that sparse momentum is robust and easy to use.\\n\\nPublished: 2022-06-20\\nTitle: 8-bit Optimizers via Block-wise Quantization\\nAuthors: Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer\\nSummary: Stateful optimizers maintain gradient statistics over time, e.g., the\\nexponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past\\ngradient values. This state can be used to accelerate optimization compared to\\nplain stochastic gradient descent but uses memory that might otherwise be\\nallocated to model parameters, thereby limiting the maximum size of models\\ntrained in practice. In this paper, we develop the first optimizers that use\\n8-bit statistics while maintaining the performance levels of using 32-bit\\noptimizer states. To overcome the resulting computational, quantization, and\\nstability challenges, we develop block-wise dynamic quantization. Block-wise\\nquantization divides input tensors into smaller blocks that are independently\\nquantized. Each block is processed in parallel across cores, yielding faster\\noptimization and high precision quantization. To maintain stability and\\nperformance, we combine block-wise quantization with two additional changes:\\n(1) dynamic quantization, a form of non-linear optimization that is precise for\\nboth large and small magnitude values, and (2) a stable embedding layer to\\nreduce gradient variance that comes from the highly non-uniform distribut', name='arxiv'),\n",
              "  AIMessage(content=\"QLoRA (Quantized Low-Rank Adapters) is an efficient finetuning approach in machine learning that reduces memory usage significantly, allowing for the finetuning of large language models on a single GPU while preserving performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). This approach has been shown to outperform previous models on benchmarks and provides state-of-the-art results with reduced memory requirements.\\n\\nHere are some papers related to QLoRA:\\n1. **QLoRA: Efficient Finetuning of Quantized LLMs**\\n   - Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\n   - Summary: Introduces the QLoRA approach for efficient finetuning of large language models with reduced memory usage.\\n   \\n2. **Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI**\\n   - Author: Dipankar Sarkar\\n   - Summary: Introduces the Viz system, which integrates Quantized Low-Rank Adapters (QLoRA) for fine-tuning large language models within a legally compliant marketplace.\\n   \\n3. **Accurate LoRA-Finetuning Quantization of LLMs via Information Retention**\\n   - Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\n   - Summary: Proposes IR-QLoRA for accurate finetuning of quantized LLMs with LoRA through information retention.\\n\\nNow, let's look up the bio of the first author, Tim Dettmers, on the QLoRA paper.\", additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 390, 'prompt_tokens': 2030, 'total_tokens': 2420}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Published: 2016-02-19\\nTitle: 8-Bit Approximations for Parallelism in Deep Learning\\nAuthors: Tim Dettmers\\nSummary: The creation of practical deep learning data-products often requires\\nparallelization across processors and computers to make deep learning feasible\\non large data sets, but bottlenecks in communication bandwidth make it\\ndifficult to attain good speedups through parallelism. Here we develop and test\\n8-bit approximation algorithms which make better use of the available bandwidth\\nby compressing 32-bit gradients and nonlinear activations to 8-bit\\napproximations. We show that these approximations do not decrease predictive\\nperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelism\\nand provide a data transfer speedup of 2x relative to 32-bit parallelism. We\\nbuild a predictive model for speedups based on our experimental data, verify\\nits validity on known speedup data, and show that we can obtain a speedup of\\n50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We\\ncompare our data types with other methods and show that 8-bit approximations\\nachieve state-of-the-art speedups for model parallelism. Thus 8-bit\\napproximation is an efficient method to parallelize convolutional networks on\\nvery large systems of GPUs.\\n\\nPublished: 2019-08-23\\nTitle: Sparse Networks from Scratch: Faster Training without Losing Performance\\nAuthors: Tim Dettmers, Luke Zettlemoyer\\nSummary: We demonstrate the possibility of what we call sparse learning: accelerated\\ntraining of deep neural networks that maintain sparse weights throughout\\ntraining while achieving dense performance levels. We accomplish this by\\ndeveloping sparse momentum, an algorithm which uses exponentially smoothed\\ngradients (momentum) to identify layers and weights which reduce the error\\nefficiently. Sparse momentum redistributes pruned weights across layers\\naccording to the mean momentum magnitude of each layer. Within a layer, sparse\\nmomentum grows weights according to the momentum magnitude of zero-valued\\nweights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10,\\nand ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared\\nto other sparse algorithms. Furthermore, we show that sparse momentum reliably\\nreproduces dense performance levels while providing up to 5.61x faster\\ntraining. In our analysis, ablations show that the benefits of momentum\\nredistribution and growth increase with the depth and size of the network.\\nAdditionally, we find that sparse momentum is insensitive to the choice of its\\nhyperparameters suggesting that sparse momentum is robust and easy to use.\\n\\nPublished: 2022-06-20\\nTitle: 8-bit Optimizers via Block-wise Quantization\\nAuthors: Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer\\nSummary: Stateful optimizers maintain gradient statistics over time, e.g., the\\nexponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past\\ngradient values. This state can be used to accelerate optimization compared to\\nplain stochastic gradient descent but uses memory that might otherwise be\\nallocated to model parameters, thereby limiting the maximum size of models\\ntrained in practice. In this paper, we develop the first optimizers that use\\n8-bit statistics while maintaining the performance levels of using 32-bit\\noptimizer states. To overcome the resulting computational, quantization, and\\nstability challenges, we develop block-wise dynamic quantization. Block-wise\\nquantization divides input tensors into smaller blocks that are independently\\nquantized. Each block is processed in parallel across cores, yielding faster\\noptimization and high precision quantization. To maintain stability and\\nperformance, we combine block-wise quantization with two additional changes:\\n(1) dynamic quantization, a form of non-linear optimization that is precise for\\nboth large and small magnitude values, and (2) a stable embedding layer to\\nreduce gradient variance that comes from the highly non-uniform distribut', name='arxiv'),\n",
              "  AIMessage(content='Tim Dettmers is a researcher who has contributed to various papers in the field of deep learning and machine learning. Some of his notable works include the development of 8-bit approximation algorithms for parallelism in deep learning, sparse networks for faster training without losing performance, and the creation of 8-bit optimizers via block-wise quantization.\\n\\nIf you would like more specific information about Tim Dettmers or his background, feel free to ask!', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 3274, 'total_tokens': 3364}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Here's a breakdown of the steps:\n",
        "\n",
        "1. Understand the user's request\n",
        "2. Initial query for QLoRA information\n",
        "3. Fetched information for Arxiv\n",
        "4. Identified the first author\n",
        "5. Conducted  a secondary query for author information\n",
        "6. Retrieve author's background and contributions\n",
        "7. Compiled and presented information"
      ],
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing for LangSmith"
      ],
      "metadata": {
        "id": "pV3XeFOT1Sar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ],
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "a8d3b5cc-ee4a-428f-b4ac-cda461e6948e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is an AI framework for improving the quality of responses generated by Large Language Models (LLMs) by grounding the model on external sources of knowledge to supplement the model's internal representation of information. RAG works with pretrained LLMs and external data to generate more accurate and reliable responses. It helps in checking claims, clearing up ambiguity, and reducing hallucination in queries. RAG is continuously evolving and is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating An Evaluation Dataset\n"
      ],
      "metadata": {
        "id": "f9UkCIqkpyZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ],
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "While the method provided is straightforward and efficient for associating questions with their answers based on list positions, it assumes a strict one-to-one correspondence and order matching between the two lists. For applications where accuracy and data integrity are paramount, additional validation, error handling, and more robust data structuring mechanisms should be considered to ensure the correct associations are maintained and clearly understood."
      ],
      "metadata": {
        "id": "ciV73F9Q04w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding Evaluators\n"
      ],
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ],
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "For a more sophisticated evaluation, consider the context in which phrases are mentioned. The method strictly checks for exact matches of phrases in the prediction. In some contexts, allowing partial matches or using fuzzy matching could make the evaluation more flexible and accurate, especially if typos or slight variations in wording are expected. Also, introducing a more granular scoring system, such as the proportion of required phrases found in the prediction, could provide more detailed feedback on the model's performance."
      ],
      "metadata": {
        "id": "PNtHORUh0jZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "e5092962-c87b-43e2-9248-4bd0bd620e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - c6779e19' at:\n",
            "https://smith.langchain.com/o/151fde86-5fb9-523a-b8a7-240c9ecb04b7/datasets/5c6566e7-4375-4d6f-b051-59bc3f5665f8/compare?selectedSessions=44120d62-de11-4620-b350-39668d73156e\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - b621fd0a at:\n",
            "https://smith.langchain.com/o/151fde86-5fb9-523a-b8a7-240c9ecb04b7/datasets/5c6566e7-4375-4d6f-b051-59bc3f5665f8\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.COT Contextual Accuracy feedback.must_mention error  execution_time                                run_id\n",
            "count                   6.00                              5.00                     6     0            6.00                                     6\n",
            "unique                   NaN                               NaN                     2     0             NaN                                     6\n",
            "top                      NaN                               NaN                 False   NaN             NaN  5fb37c39-f67e-4342-ba25-277fa15befc4\n",
            "freq                     NaN                               NaN                     3   NaN             NaN                                     1\n",
            "mean                    0.83                              0.60                   NaN   NaN            3.33                                   NaN\n",
            "std                     0.41                              0.55                   NaN   NaN            1.09                                   NaN\n",
            "min                     0.00                              0.00                   NaN   NaN            1.29                                   NaN\n",
            "25%                     1.00                              0.00                   NaN   NaN            3.29                                   NaN\n",
            "50%                     1.00                              1.00                   NaN   NaN            3.55                                   NaN\n",
            "75%                     1.00                              1.00                   NaN   NaN            3.84                                   NaN\n",
            "max                     1.00                              1.00                   NaN   NaN            4.47                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - c6779e19',\n",
              " 'results': {'af6f0887-27d0-4e48-ab77-960fe960f5e9': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides information about QLoRA, including the fact that it uses 4-bit quantization to compress a pretrained language model and that it uses the Transformers library for handling pre-trained language models and fine-tuning. \\n\\nHowever, the question asked specifically about the optimizer used in QLoRA. The submission does not provide a direct answer to this question. Instead, it states that the optimizer is not explicitly mentioned in the search results. \\n\\nWhile the additional information provided might be useful in a broader context, it does not directly answer the question asked. Therefore, it may not be considered helpful in this specific context.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('eb80fec5-6fdd-4df0-b30c-ac31f6aa60c4'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer does not directly answer the question about the specific optimizer used in QLoRA. However, the student does provide relevant information about QLoRA's use of 4-bit quantization and the Transformers library. The context provided does not give any information about the optimizer used in QLoRA. Therefore, based on the information provided, the student's answer cannot be definitively marked as correct or incorrect. \\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('12a26b6d-bf8f-4dab-9671-20c2e2e9c257'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('941e0325-9b56-48f6-957f-b0643cb359a7'), target_run_id=None)],\n",
              "   'execution_time': 3.2606,\n",
              "   'run_id': '5fb37c39-f67e-4342-ba25-277fa15befc4',\n",
              "   'output': 'The optimizer used in QLoRA is not explicitly mentioned in the search results. However, it is mentioned that QLoRA uses 4-bit quantization to compress a pretrained language model. Additionally, the search results indicate that QLoRA uses Transformers library for handling pre-trained language models and fine-tuning.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '53faf2df-5250-4f63-9900-dce5cf79ce9b': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question, stating that the QLoRA paper introduced a new data type called \"4-bit NormalFloat (NF4)\". This is helpful as it directly answers the question asked in the input. \\n\\nThe submission also provides additional information, stating that this new data type is \"information theoretically optimal for normally distributed weights\". This is insightful as it provides more context about the data type and why it was created. \\n\\nThe submission is appropriate as it is relevant to the question and provides a clear and concise answer. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6a094355-7e94-4529-b5ef-d874a96cfb40'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'NormalFloat' as the data type created in the QLoRA paper. The student's answer also mentions '4-bit NormalFloat (NF4)' as the data type created in the QLoRA paper. The student's answer matches the context and provides additional information about the data type being optimal for normally distributed weights. There is no conflicting information in the student's answer.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('75de403b-8808-4ffb-8c1b-1ebdcc5ab67e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('91122fef-cf75-4383-a7c0-1da8f32a64e4'), target_run_id=None)],\n",
              "   'execution_time': 3.883055,\n",
              "   'run_id': '6ba5c928-e204-4c66-b160-da2fbd3cb017',\n",
              "   'output': 'The QLoRA paper introduced a new data type called \"4-bit NormalFloat (NF4)\" which is information theoretically optimal for normally distributed weights.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '030b187f-11df-4783-a18b-bdf60efc03f4': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains that RAG is an architecture that enhances the capabilities of a Large Language Model (LLM) by incorporating an information retrieval system. It further explains how this system works and the benefits it provides, such as enriching the context and content of the language model\\'s response and grounding the model on external sources of knowledge. \\n\\nThe submission is insightful as it provides a clear and comprehensive understanding of the topic. It is also appropriate as it directly answers the question asked in the input. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('97e2ae10-4f7a-4ab2-80a1-73081ef07137'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The answer is in line with the context provided, explaining that RAG is an architecture that enhances the capabilities of a Large Language Model (LLM) by incorporating an information retrieval system. The student also correctly states that RAG is used to improve the quality of LLM-generated responses by grounding the model on external sources of knowledge. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('52a2cc99-3f5b-4bad-85f1-56ac809709a5'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('8785cba8-bcaf-4ce0-9675-8cccd0d0d93f'), target_run_id=None)],\n",
              "   'execution_time': 3.698762,\n",
              "   'run_id': '28368cbd-c488-447c-b299-0f581ca420d2',\n",
              "   'output': \"Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data. This system enhances traditional language model responses by incorporating real-time, external data retrieval, enriching the context and content of the language model's response. RAG is a technique used to improve the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information.\",\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '442fd267-a845-4b4a-8f7b-72dc25411ce6': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to be helpful, insightful, and appropriate. \\n\\nLooking at the input, the question asked is about the authorship of the QLoRA paper. \\n\\nThe submission provided a list of authors, but it refers to a different paper, the QDyLoRA paper, not the QLoRA paper as asked in the input. \\n\\nThis means that the submission is not appropriate to the question asked, and therefore, it is not helpful or insightful in this context. \\n\\nSo, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fb0ae428-69e9-40f4-901f-988122e8dd8c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer does not match the context provided. The context states that the authors of the QLoRA paper are 'Tim' and 'Dettmers', but the student has listed a completely different set of authors. Therefore, the student's answer is incorrect.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a997386e-1f7e-4374-9376-0daa067f94bd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('5c948d0d-0d6b-4625-8d2b-99f67d6cbc64'), target_run_id=None)],\n",
              "   'execution_time': 4.472772,\n",
              "   'run_id': 'ee296bf6-8982-4573-8c97-3a0cadd30bbb',\n",
              "   'output': 'The QDyLoRA paper was authored by Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  'b4b451e8-0958-4286-a2c6-eb16073cd0e2': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear answer to the question, stating that TensorFlow is the most popular deep learning framework. It also provides additional information about TensorFlow, such as its open-source nature, its developer (Google), and its wide usage in building and training deep learning models. \\n\\nThe submission also mentions the comprehensive ecosystem of tools, libraries, and community support that TensorFlow provides, which adds to its helpfulness by giving the reader a better understanding of why TensorFlow is popular.\\n\\nFurthermore, the submission mentions other popular deep learning frameworks, which provides the reader with alternatives to TensorFlow. This is also helpful as it gives the reader a broader perspective on the topic.\\n\\nBased on this analysis, the submission is helpful, insightful, and appropriate. Therefore, it meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f2a5ecfe-f636-4237-b2a8-661da981787e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer states that TensorFlow is the most popular deep learning framework. This aligns with the context provided, which lists TensorFlow as one of the popular deep learning frameworks. The student also provides additional information about TensorFlow, which does not contradict the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2703c659-ee43-45bd-b28f-54ad7dfb5758'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('703da2b0-f0a8-40cf-9c0f-d20428e7d6cd'), target_run_id=None)],\n",
              "   'execution_time': 1.28816,\n",
              "   'run_id': '5b43ea26-b547-4743-bf21-22a16504c47e',\n",
              "   'output': 'The most popular deep learning framework is TensorFlow. TensorFlow is an open-source machine learning library developed by Google that is widely used for building and training deep learning models. It provides a comprehensive ecosystem of tools, libraries, and community support for deep learning research and applications. Other popular deep learning frameworks include PyTorch, Keras, and Caffe.',\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '4610a00d-8d0e-4399-9d29-826e3ce53f6a': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nLooking at the submission, it provides a detailed explanation of the improvements that the LoRa system makes. It mentions long-distance communication, low-power consumption, simultaneous transmissions, and energy efficiency as the significant improvements. \\n\\nThe submission also explains how these improvements enable versatile IoT deployments, efficient data collection from sensors over large geographical areas, and high throughput with low overhead. \\n\\nFurthermore, it provides additional information about how the LoRa system optimizes data transmission for low-power wide-area network applications, making it a popular choice for modern wireless communication technologies. \\n\\nBased on these points, the submission appears to be helpful, insightful, and appropriate. It provides a comprehensive answer to the question and offers additional context that could be useful to the reader. \\n\\nTherefore, the submission meets the criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('df060905-25ef-40f9-920a-38598ac9f403'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context provided does not give any information about the LoRA system or its improvements. Therefore, it is impossible to verify the accuracy of the student's answer based on the given context.\\nGRADE: INCONCLUSIVE\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('36bf97cc-3daf-484f-8171-06557dfc794b'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=UUID('cdee5d7b-aac9-4a9a-a344-38fa7ee16ac4'), target_run_id=None)],\n",
              "   'execution_time': 3.39185,\n",
              "   'run_id': '7e184b2f-5934-4bcb-a835-03685897fa3e',\n",
              "   'output': 'The LoRa system offers significant improvements such as long-distance communication, low-power consumption, simultaneous transmissions, and energy efficiency. It enables versatile IoT deployments, efficient data collection from sensors over large geographical areas, and high throughput with low overhead. Additionally, the LoRa system optimizes data transmission for low-power wide-area network applications, making it a popular choice for modern wireless communication technologies.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}